use gillean::developer_tools::{
    DeveloperToolsManager, Debugger, SDKGenerator, MonitoringDashboard, CodeAnalyzer,
    DeveloperToolsConfig, DebuggerConfig, SDKGeneratorConfig, MonitoringConfig, CodeAnalysisConfig,
    DebugLevel, SDKLanguage, WidgetType, SDKTemplate, AlertSeverity, Widget
};
use std::sync::Arc;
use std::time::Duration;


pub struct DeveloperToolsTestSuite {
    manager: Arc<DeveloperToolsManager>,
}

impl DeveloperToolsTestSuite {
    pub fn new() -> Self {
        let config = DeveloperToolsConfig {
            enable_debugging: true,
            enable_sdk_generation: true,
            enable_monitoring: true,
            enable_code_analysis: true,
            debug_level: DebugLevel::Info,
            sdk_languages: vec![SDKLanguage::Rust, SDKLanguage::TypeScript],
            monitoring_interval: Duration::from_secs(30),
        };

        Self {
            manager: Arc::new(DeveloperToolsManager::new(config)),
        }
    }

    pub async fn run_all_tests(&self) -> Result<(), String> {
        println!("ðŸ› ï¸ Running Developer Tools tests...");

        // Initialize the developer tools manager
        self.manager.initialize().await?;

        self.test_debugger().await?;
        self.test_sdk_generator().await?;
        self.test_monitoring_dashboard().await?;
        self.test_code_analyzer().await?;
        self.test_developer_tools_manager().await?;
        self.test_developer_report().await?;

        println!("  âœ… Developer Tools tests completed!");
        Ok(())
    }

    async fn test_debugger(&self) -> Result<(), String> {
        println!("    Testing Debugger...");

        let debugger_config = DebuggerConfig {
            max_log_entries: 100,
            enable_call_stack_tracking: true,
            enable_variable_watching: true,
            log_retention_period: Duration::from_secs(60),
        };
        let debugger = Debugger::new(debugger_config);

        // Test breakpoint management
        let _breakpoint_id = debugger.add_breakpoint("main.rs:10", Some("x > 5")).await;
        debugger.add_breakpoint("utils.rs:25", None).await;

        // Test debug logging
        debugger.log_debug(DebugLevel::Info, "Test info message", "test").await;
        debugger.log_debug(DebugLevel::Warning, "Test warning message", "test").await;
        debugger.log_debug(DebugLevel::Error, "Test error message", "test").await;

        // Test call stack tracking
        debugger.add_call_stack_frame("main", "main.rs", 1).await;
        debugger.add_call_stack_frame("process_data", "utils.rs", 15).await;

        // Test variable watching
        debugger.set_variable("x", "42", "i32", "main").await;
        debugger.set_variable("y", "3.14", "f64", "main").await;

        // Test debug info retrieval
        let debug_info = debugger.get_debug_info().await;
        assert_eq!(debug_info.breakpoints.len(), 2);
        assert_eq!(debug_info.debug_logs.len(), 3);
        assert_eq!(debug_info.call_stack.len(), 2);
        assert_eq!(debug_info.variables.len(), 2);

        // Test stepping through code
        let step_result = debugger.step_through("test_function").await;
        assert_eq!(step_result.current_function, "test_function");

        println!("      âœ… Debugger tests passed");
        Ok(())
    }

    async fn test_sdk_generator(&self) -> Result<(), String> {
        println!("    Testing SDK Generator...");

        let sdk_config = SDKGeneratorConfig {
            output_directory: "./test_sdks".to_string(),
            enable_auto_generation: true,
            template_directory: "./test_templates".to_string(),
        };
        let generator = SDKGenerator::new(sdk_config);

        // Add SDK templates
        let rust_template = SDKTemplate {
            template_id: "rust_sdk".to_string(),
            language: SDKLanguage::Rust,
            template_content: r#"
pub struct GilleanSDK {
    pub client: reqwest::Client,
    pub base_url: String,
}

impl GilleanSDK {
    pub fn new(base_url: String) -> Self {
        Self {
            client: reqwest::Client::new(),
            base_url,
        }
    }
}
"#.to_string(),
            dependencies: vec!["reqwest".to_string(), "serde".to_string()],
            version: "1.0.0".to_string(),
        };

        let typescript_template = SDKTemplate {
            template_id: "typescript_sdk".to_string(),
            language: SDKLanguage::TypeScript,
            template_content: r#"
export class GilleanSDK {
    private baseUrl: string;
    private client: AxiosInstance;

    constructor(baseUrl: string) {
        this.baseUrl = baseUrl;
        this.client = axios.create({ baseURL: baseUrl });
    }
}
"#.to_string(),
            dependencies: vec!["axios".to_string()],
            version: "1.0.0".to_string(),
        };

        generator.add_template(rust_template).await;
        generator.add_template(typescript_template).await;

        // Test SDK generation
        let api_spec = r#"{
            "openapi": "3.0.0",
            "info": {
                "title": "Gillean Blockchain API",
                "version": "1.0.0"
            },
            "paths": {
                "/blocks": {
                    "get": {
                        "summary": "Get blockchain blocks"
                    }
                }
            }
        }"#;

        let rust_sdk = generator.generate_sdk(SDKLanguage::Rust, api_spec).await?;
        let typescript_sdk = generator.generate_sdk(SDKLanguage::TypeScript, api_spec).await?;

        assert_eq!(rust_sdk.language, SDKLanguage::Rust);
        assert_eq!(typescript_sdk.language, SDKLanguage::TypeScript);
        assert!(rust_sdk.file_count > 0);
        assert!(typescript_sdk.file_count > 0);

        // Test template management
        let templates = generator.get_templates().await;
        assert_eq!(templates.len(), 2);

        let generated_sdks = generator.get_generated_sdks().await;
        assert_eq!(generated_sdks.len(), 2);

        println!("      âœ… SDK Generator tests passed");
        Ok(())
    }

    async fn test_monitoring_dashboard(&self) -> Result<(), String> {
        println!("    Testing Monitoring Dashboard...");

        let monitoring_config = MonitoringConfig {
            metrics_retention_period: Duration::from_secs(60),
            alert_check_interval: Duration::from_secs(10),
            dashboard_refresh_interval: Duration::from_secs(5),
        };
        let dashboard = MonitoringDashboard::new(monitoring_config);

        // Test metrics recording
        let mut tags = std::collections::HashMap::new();
        tags.insert("service".to_string(), "test".to_string());
        tags.insert("environment".to_string(), "development".to_string());

        dashboard.record_metric("cpu_usage", 75.5, "percent", tags.clone()).await;
        dashboard.record_metric("memory_usage", 2.5, "GB", tags.clone()).await;
        dashboard.record_metric("request_count", 1000.0, "requests", tags).await;

        // Test alert creation
        dashboard.create_alert("High CPU", AlertSeverity::Warning, "CPU usage is high").await;
        dashboard.create_alert("Memory Full", AlertSeverity::Error, "Memory usage is critical").await;
        dashboard.create_alert("System Down", AlertSeverity::Critical, "System is down").await;

        // Test dashboard creation
        let dashboard_id = dashboard.create_dashboard("Test Dashboard", "A test monitoring dashboard").await;

        // Test widget addition
        let metrics_widget = Widget {
            id: "metrics_widget".to_string(),
            widget_type: WidgetType::LineChart,
            title: "System Metrics".to_string(),
            config: std::collections::HashMap::new(),
        };

        let alerts_widget = Widget {
            id: "alerts_widget".to_string(),
            widget_type: WidgetType::Table,
            title: "Active Alerts".to_string(),
            config: std::collections::HashMap::new(),
        };

        dashboard.add_widget(&dashboard_id, metrics_widget).await?;
        dashboard.add_widget(&dashboard_id, alerts_widget).await?;

        // Test dashboard data retrieval
        let dashboard_data = dashboard.get_dashboard_data(&dashboard_id).await;
        assert!(dashboard_data.is_some());

        // Test monitoring summary
        let summary = dashboard.get_monitoring_summary().await;
        assert_eq!(summary.total_metrics, 3);
        assert_eq!(summary.active_alerts, 3);
        assert_eq!(summary.total_dashboards, 1);

        // Test alert resolution
        // Note: get_detected_threats method not available in current implementation
        // let alerts = dashboard.get_detected_threats().await;
        // if !alerts.is_empty() {
        //     dashboard.resolve_alert(&alerts[0].threat_id).await?;
        // }

        println!("      âœ… Monitoring Dashboard tests passed");
        Ok(())
    }

    async fn test_code_analyzer(&self) -> Result<(), String> {
        println!("    Testing Code Analyzer...");

        let analysis_config = CodeAnalysisConfig {
            enable_security_analysis: true,
            enable_performance_analysis: true,
            enable_quality_analysis: true,
            analysis_timeout: Duration::from_secs(60),
        };
        let analyzer = CodeAnalyzer::new(analysis_config);

        // Test code analysis
        let safe_code = r#"
        fn safe_function() {
            let x = 5;
            let y = x + 1;
            println!("Result: {}", y);
        }

        #[test]
        fn test_safe_function() {
            safe_function();
        }
        "#;

        let unsafe_code = r#"
        fn unsafe_function() {
            let password = "plaintext_password";
            unsafe {
                let ptr = std::ptr::null_mut();
                *ptr = 42;
            }
        }
        "#;

        let complex_code = r#"
        fn complex_function() {
            for i in 0..100 {
                for j in 0..100 {
                    for k in 0..100 {
                        println!("{} {} {}", i, j, k);
                    }
                }
            }
        }
        "#;

        // Analyze different code samples
        let safe_analysis = analyzer.analyze_code("safe.rs", safe_code).await;
        let unsafe_analysis = analyzer.analyze_code("unsafe.rs", unsafe_code).await;
        let _complex_analysis = analyzer.analyze_code("complex.rs", complex_code).await;

        // Test that unsafe code has more findings
        assert!(unsafe_analysis.findings.len() >= safe_analysis.findings.len());

        // Test metrics calculation
        let safe_metrics = analyzer.calculate_metrics("safe.rs", safe_code).await;
        let unsafe_metrics = analyzer.calculate_metrics("unsafe.rs", unsafe_code).await;
        let complex_metrics = analyzer.calculate_metrics("complex.rs", complex_code).await;

        // Verify metrics
        assert!(safe_metrics.security_score > unsafe_metrics.security_score);
        assert!(complex_metrics.cyclomatic_complexity > safe_metrics.cyclomatic_complexity);

        // Test analysis results retrieval
        let analysis_results = analyzer.get_analysis_results().await;
        assert_eq!(analysis_results.len(), 3);

        let code_metrics = analyzer.get_code_metrics().await;
        assert_eq!(code_metrics.len(), 3);

        println!("      âœ… Code Analyzer tests passed");
        Ok(())
    }

    async fn test_developer_tools_manager(&self) -> Result<(), String> {
        println!("    Testing Developer Tools Manager...");

        // Test developer tools status
        let _status = self.manager.get_developer_tools_status().await;
        // len() returns usize (always >= 0), total_metrics is usize (always >= 0)

        println!("      âœ… Developer Tools Manager tests passed");
        Ok(())
    }

    async fn test_developer_report(&self) -> Result<(), String> {
        println!("    Testing Developer Report...");

        // Generate developer report
        let report = self.manager.generate_developer_report().await;
        assert!(report.duration > Duration::from_nanos(0));
        // len() returns usize (always >= 0), total_metrics is usize (always >= 0)

        // Test recommendations
        println!("      Report recommendations: {:?}", report.recommendations);

        println!("      âœ… Developer Report tests passed");
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_developer_tools_suite() {
        let suite = DeveloperToolsTestSuite::new();
        suite.run_all_tests().await.unwrap();
    }

    #[tokio::test]
    async fn test_debugger_functionality() {
        let config = DebuggerConfig {
            max_log_entries: 50,
            enable_call_stack_tracking: true,
            enable_variable_watching: true,
            log_retention_period: Duration::from_secs(30),
        };
        let debugger = Debugger::new(config);

        // Test comprehensive debugging workflow
        let _breakpoint_id = debugger.add_breakpoint("main.rs:42", Some("x > 10")).await;
        
        debugger.log_debug(DebugLevel::Debug, "Starting function execution", "main").await;
        debugger.add_call_stack_frame("main", "main.rs", 1).await;
        debugger.set_variable("x", "15", "i32", "main").await;
        
        debugger.log_debug(DebugLevel::Info, "Variable x set to 15", "main").await;
        debugger.add_call_stack_frame("process", "utils.rs", 10).await;
        debugger.set_variable("result", "25", "i32", "process").await;
        
        debugger.log_debug(DebugLevel::Debug, "Function execution completed", "main").await;

        let debug_info = debugger.get_debug_info().await;
        assert_eq!(debug_info.breakpoints.len(), 1);
        assert_eq!(debug_info.debug_logs.len(), 3);
        assert_eq!(debug_info.call_stack.len(), 2);
        assert_eq!(debug_info.variables.len(), 2);
    }

    #[tokio::test]
    async fn test_sdk_generation_workflow() {
        let config = SDKGeneratorConfig {
            output_directory: "./test_output".to_string(),
            enable_auto_generation: true,
            template_directory: "./test_templates".to_string(),
        };
        let generator = SDKGenerator::new(config);

        // Test multiple SDK generation
        let languages = vec![SDKLanguage::Rust, SDKLanguage::TypeScript, SDKLanguage::Python];
        
        for language in languages {
            let template = SDKTemplate {
                template_id: format!("{}_template", language),
                language,
                template_content: format!("// {} SDK template", language),
                dependencies: vec!["test".to_string()],
                version: "1.0.0".to_string(),
            };
            
            generator.add_template(template).await;
            
            let api_spec = r#"{"endpoints": []}"#;
            let sdk = generator.generate_sdk(language, api_spec).await.unwrap();
            assert_eq!(sdk.language, language);
        }

        let generated_sdks = generator.get_generated_sdks().await;
        assert_eq!(generated_sdks.len(), 3);
    }

    #[tokio::test]
    async fn test_monitoring_comprehensive() {
        let config = MonitoringConfig {
            metrics_retention_period: Duration::from_secs(60),
            alert_check_interval: Duration::from_secs(5),
            dashboard_refresh_interval: Duration::from_secs(2),
        };
        let dashboard = MonitoringDashboard::new(config);

        // Simulate comprehensive monitoring scenario
        for i in 0..10 {
            let mut tags = std::collections::HashMap::new();
            tags.insert("service".to_string(), format!("service_{}", i));
            tags.insert("instance".to_string(), format!("instance_{}", i));
            
            dashboard.record_metric("cpu_usage", 50.0 + i as f64, "percent", tags.clone()).await;
            dashboard.record_metric("memory_usage", 1.0 + i as f64 * 0.1, "GB", tags.clone()).await;
            dashboard.record_metric("request_rate", 100.0 + i as f64 * 10.0, "req/s", tags).await;
        }

        // Create various alerts
        let alert_severities = vec![
            AlertSeverity::Info,
            AlertSeverity::Warning,
            AlertSeverity::Error,
            AlertSeverity::Critical,
        ];

        for (i, severity) in alert_severities.iter().enumerate() {
            dashboard.create_alert(
                &format!("Alert {}", i),
                severity.clone(),
                &format!("Test alert {}", i)
            ).await;
        }

        let summary = dashboard.get_monitoring_summary().await;
        assert_eq!(summary.total_metrics, 30); // 10 services * 3 metrics
        assert_eq!(summary.active_alerts, 4);
    }

    #[tokio::test]
    async fn test_code_analysis_comprehensive() {
        let config = CodeAnalysisConfig {
            enable_security_analysis: true,
            enable_performance_analysis: true,
            enable_quality_analysis: true,
            analysis_timeout: Duration::from_secs(30),
        };
        let analyzer = CodeAnalyzer::new(config);

        // Test various code patterns
        let test_codes = vec![
            ("secure_code.rs", r#"
                fn secure_function() {
                    let hashed_password = hash_password("password");
                    validate_input(input);
                }
            "#),
            ("performance_code.rs", r#"
                fn optimized_function() {
                    let data = Vec::with_capacity(1000);
                    for i in 0..1000 {
                        data.push(i);
                    }
                }
            "#),
            ("quality_code.rs", r#"
                fn well_structured_function() {
                    let result = process_data();
                    if result.is_ok() {
                        handle_success(result.unwrap());
                    } else {
                        handle_error(result.unwrap_err());
                    }
                }
            "#),
        ];

        for (filename, code) in test_codes {
            let _analysis = analyzer.analyze_code(filename, code).await;
            let metrics = analyzer.calculate_metrics(filename, code).await;
            
            // findings.len() returns usize (always >= 0)
            assert!(metrics.lines_of_code > 0);
            assert!(metrics.security_score >= 0.0);
            assert!(metrics.maintainability_index >= 0.0);
        }
    }
}
